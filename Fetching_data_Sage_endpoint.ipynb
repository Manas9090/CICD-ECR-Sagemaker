{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02562a28-d8b6-4a99-98de-370e313419c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.7.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n",
      "----!"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "from sagemaker.model import Model\n",
    "from sagemaker import get_execution_role, Session\n",
    "\n",
    "role = get_execution_role()\n",
    "session = Session()\n",
    "\n",
    "ecr_image = '491085388405.dkr.ecr.us-east-1.amazonaws.com/fraud-ecr-27:latest'\n",
    "\n",
    "model = Model(\n",
    "    image_uri=ecr_image,\n",
    "    role=role,\n",
    "    sagemaker_session=session\n",
    ")\n",
    "\n",
    "predictor = model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.m5.large',\n",
    "    endpoint_name='custom-ml-endpoint'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a82b6e4f-b21c-45dc-bd05-cbd1c5f36609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: psycopg2-binary in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (2.9.9)\n",
      "Requirement already satisfied: boto3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (1.38.38)\n",
      "Requirement already satisfied: sqlalchemy in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (2.0.41)\n",
      "Requirement already satisfied: psycopg2==2.9.9 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from psycopg2-binary) (2.9.9)\n",
      "Requirement already satisfied: botocore<1.39.0,>=1.38.38 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from boto3) (1.38.38)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from boto3) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.14.0,>=0.13.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from boto3) (0.13.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from botocore<1.39.0,>=1.38.38->boto3) (2.9.0.post0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from botocore<1.39.0,>=1.38.38->boto3) (2.4.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.39.0,>=1.38.38->boto3) (1.17.0)\n",
      "Requirement already satisfied: greenlet>=1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sqlalchemy) (3.2.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sqlalchemy) (4.14.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install psycopg2-binary boto3 sqlalchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89e43c03-1605-4983-8ca5-24225da3a8e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting supabase\n",
      "  Downloading supabase-2.17.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting gotrue==2.12.3 (from supabase)\n",
      "  Downloading gotrue-2.12.3-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting httpx<0.29,>=0.26 (from supabase)\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting postgrest==1.1.1 (from supabase)\n",
      "  Downloading postgrest-1.1.1-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting realtime==2.6.0 (from supabase)\n",
      "  Downloading realtime-2.6.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting storage3==0.12.0 (from supabase)\n",
      "  Downloading storage3-0.12.0-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting supafunc==0.10.1 (from supabase)\n",
      "  Downloading supafunc-0.10.1-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: pydantic<3,>=1.10 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gotrue==2.12.3->supabase) (2.9.2)\n",
      "Requirement already satisfied: pyjwt<3.0.0,>=2.10.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gotrue==2.12.3->supabase) (2.10.1)\n",
      "Collecting deprecation<3.0.0,>=2.1.0 (from postgrest==1.1.1->supabase)\n",
      "  Downloading deprecation-2.1.0-py2.py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting strenum<0.5.0,>=0.4.9 (from postgrest==1.1.1->supabase)\n",
      "  Downloading StrEnum-0.4.15-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting pydantic<3,>=1.10 (from gotrue==2.12.3->supabase)\n",
      "  Using cached pydantic-2.11.7-py3-none-any.whl.metadata (67 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.14.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from realtime==2.6.0->supabase) (4.14.0)\n",
      "Collecting websockets<16,>=11 (from realtime==2.6.0->supabase)\n",
      "  Downloading websockets-15.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.8.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from storage3==0.12.0->supabase) (2.9.0.post0)\n",
      "Requirement already satisfied: packaging in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from deprecation<3.0.0,>=2.1.0->postgrest==1.1.1->supabase) (24.2)\n",
      "Requirement already satisfied: anyio in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from httpx<0.29,>=0.26->supabase) (4.9.0)\n",
      "Requirement already satisfied: certifi in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from httpx<0.29,>=0.26->supabase) (2025.6.15)\n",
      "Collecting httpcore==1.* (from httpx<0.29,>=0.26->supabase)\n",
      "  Using cached httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: idna in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from httpx<0.29,>=0.26->supabase) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from httpcore==1.*->httpx<0.29,>=0.26->supabase) (0.16.0)\n",
      "Requirement already satisfied: h2<5,>=3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from httpx[http2]<0.29,>=0.26->gotrue==2.12.3->supabase) (4.2.0)\n",
      "Requirement already satisfied: hyperframe<7,>=6.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from h2<5,>=3->httpx[http2]<0.29,>=0.26->gotrue==2.12.3->supabase) (6.1.0)\n",
      "Requirement already satisfied: hpack<5,>=4.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from h2<5,>=3->httpx[http2]<0.29,>=0.26->gotrue==2.12.3->supabase) (4.1.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pydantic<3,>=1.10->gotrue==2.12.3->supabase) (0.7.0)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic<3,>=1.10->gotrue==2.12.3->supabase)\n",
      "  Using cached pydantic_core-2.33.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic<3,>=1.10->gotrue==2.12.3->supabase)\n",
      "  Using cached typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.8.2->storage3==0.12.0->supabase) (1.17.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from anyio->httpx<0.29,>=0.26->supabase) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from anyio->httpx<0.29,>=0.26->supabase) (1.3.1)\n",
      "Downloading supabase-2.17.0-py3-none-any.whl (17 kB)\n",
      "Downloading gotrue-2.12.3-py3-none-any.whl (44 kB)\n",
      "Downloading postgrest-1.1.1-py3-none-any.whl (22 kB)\n",
      "Downloading realtime-2.6.0-py3-none-any.whl (21 kB)\n",
      "Downloading storage3-0.12.0-py3-none-any.whl (18 kB)\n",
      "Downloading supafunc-0.10.1-py3-none-any.whl (8.0 kB)\n",
      "Downloading deprecation-2.1.0-py2.py3-none-any.whl (11 kB)\n",
      "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Using cached httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Using cached pydantic-2.11.7-py3-none-any.whl (444 kB)\n",
      "Using cached pydantic_core-2.33.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "Downloading StrEnum-0.4.15-py3-none-any.whl (8.9 kB)\n",
      "Downloading websockets-15.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (181 kB)\n",
      "Using cached typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: strenum, websockets, typing-inspection, pydantic-core, httpcore, deprecation, pydantic, realtime, httpx, supafunc, storage3, postgrest, gotrue, supabase\n",
      "\u001b[2K  Attempting uninstall: pydantic-core\n",
      "\u001b[2K    Found existing installation: pydantic_core 2.23.4\n",
      "\u001b[2K    Uninstalling pydantic_core-2.23.4:\n",
      "\u001b[2K      Successfully uninstalled pydantic_core-2.23.4\n",
      "\u001b[2K  Attempting uninstall: pydantic90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m 3/14\u001b[0m [pydantic-core]\n",
      "\u001b[2K    Found existing installation: pydantic 2.9.2‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m 3/14\u001b[0m [pydantic-core]\n",
      "\u001b[2K    Uninstalling pydantic-2.9.2:0m‚ï∫\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m 6/14\u001b[0m [pydantic]]\n",
      "\u001b[2K      Successfully uninstalled pydantic-2.9.2‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m 6/14\u001b[0m [pydantic]\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m14/14\u001b[0m [supabase]/14\u001b[0m [gotrue]e]\n",
      "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "safety-schemas 0.0.14 requires pydantic<2.10.0,>=2.6.0, but you have pydantic 2.11.7 which is incompatible.\n",
      "sparkmagic 0.21.0 requires pandas<2.0.0,>=0.17.1, but you have pandas 2.2.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed deprecation-2.1.0 gotrue-2.12.3 httpcore-1.0.9 httpx-0.28.1 postgrest-1.1.1 pydantic-2.11.7 pydantic-core-2.33.2 realtime-2.6.0 storage3-0.12.0 strenum-0.4.15 supabase-2.17.0 supafunc-0.10.1 typing-inspection-0.4.1 websockets-15.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install supabase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6499e5e1-496b-472f-ae30-7756a0207fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Upload to S3 complete\n"
     ]
    }
   ],
   "source": [
    "###### Fetching the Data from Supabase and storing in S3 ########\n",
    "#################################################################\n",
    "# print(\"Supabase raw response:\", response)\n",
    "# print(\"Supabase data:\", response.data)\n",
    "\n",
    "\n",
    "# print(\"DataFrame shape:\", df.shape)\n",
    "# print(\"DataFrame head:\\n\", df.head())\n",
    "\n",
    "\n",
    "from supabase import create_client, Client\n",
    "import pandas as pd\n",
    "import boto3\n",
    "\n",
    "# Supabase config\n",
    "SUPABASE_URL = \"https://mcgzvjzuqnjstptfrsuj.supabase.co\"\n",
    "SUPABASE_KEY = \"sb_publishable_4vx6B3HjwnEQVwI9SgrJkQ_S32tu5l5\"\n",
    "\n",
    "# Use the service_role key, not anon\n",
    "\n",
    "# Create Supabase client\n",
    "supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)\n",
    "\n",
    "# Fetch data\n",
    "response = supabase.table(\"loan_fraud_analytics\").select(\"*\").execute()\n",
    "df = pd.DataFrame(response.data)\n",
    "\n",
    "# Save as CSV\n",
    "csv_path = \"/tmp/loan_fraud_data.csv\"\n",
    "df.to_csv(csv_path, index=False)\n",
    "\n",
    "# Upload to S3\n",
    "s3 = boto3.client(\"s3\")\n",
    "bucket_name = \"manas-bucket100\"\n",
    "object_key = \"input-file/loan_fraud_data.csv\"\n",
    "\n",
    "s3.upload_file(csv_path, bucket_name, object_key)\n",
    "print(\"‚úÖ Upload to S3 complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe03ea4b-aa5e-49be-bc49-aaf506ca84bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Features uploaded to s3://manas-bucket100/input-file/loan_fraud_data.csv\n"
     ]
    }
   ],
   "source": [
    "######## Dropping the target variable ########\n",
    "##############################################\n",
    "import boto3\n",
    "import io\n",
    "\n",
    "# Drop target column\n",
    "df_features = df.drop(columns=['loan_default'])\n",
    "\n",
    "# Convert to CSV buffer\n",
    "csv_buffer = io.StringIO()\n",
    "df_features.to_csv(csv_buffer, index=False)\n",
    "\n",
    "# Upload to S3\n",
    "s3 = boto3.client('s3')\n",
    "bucket_name = 'manas-bucket100'  # üîÅ Your bucket name\n",
    "object_key = 'input-file/loan_fraud_data.csv'\n",
    "\n",
    "s3.put_object(Bucket=bucket_name, Key=object_key, Body=csv_buffer.getvalue())\n",
    "\n",
    "print(f\"‚úÖ Features uploaded to s3://{bucket_name}/{object_key}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc70e51a-40ea-461e-bf00-8f19dc952f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded data from s3://manas-bucket100/input-file/loan_fraud_data.csv\n",
      "üì• Original shape: (1000, 9)\n",
      "üßπ Dropping low-variance/id-like columns: ['last_delinq_none', 'loan_id']\n",
      "üìä Processed shape: (1000, 10)\n",
      "‚úÖ Preprocessed test data saved: processed_test_data.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_15001/2229654881.py:26: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(df[col].median(), inplace=True)\n",
      "/tmp/ipykernel_15001/2229654881.py:24: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(df[col].mode()[0], inplace=True)\n"
     ]
    }
   ],
   "source": [
    "####### Code for Data processing ########\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import boto3\n",
    "from io import StringIO\n",
    "bucket_name = \"manas-bucket100\"\n",
    "s3_key = \"input-file/loan_fraud_data.csv\"\n",
    "\n",
    "# Step 1: Define S3 download\n",
    "def load_csv_from_s3(bucket_name, file_key):\n",
    "    s3 = boto3.client('s3')\n",
    "    response = s3.get_object(Bucket=bucket_name, Key=file_key)\n",
    "    content = response['Body'].read().decode('utf-8')\n",
    "    df = pd.read_csv(StringIO(content))\n",
    "    print(f\"‚úÖ Loaded data from s3://{bucket_name}/{file_key}\")\n",
    "    return df\n",
    "\n",
    "# Step 2: Handle missing values\n",
    "def handle_missing_values(df):\n",
    "    df = df.copy()\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'object':\n",
    "            df[col].fillna(df[col].mode()[0], inplace=True)\n",
    "        else:\n",
    "            df[col].fillna(df[col].median(), inplace=True)\n",
    "    return df\n",
    "\n",
    "# Step 3: Handle outliers\n",
    "def handle_outliers(df):\n",
    "    df = df.copy()\n",
    "    for col in df.select_dtypes(include=np.number).columns:\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower = Q1 - 1.5 * IQR\n",
    "        upper = Q3 + 1.5 * IQR\n",
    "        df[col] = np.where(df[col] < lower, lower,\n",
    "                  np.where(df[col] > upper, upper, df[col]))\n",
    "    return df\n",
    "\n",
    "# Step 4: Create dummies\n",
    "def create_dummies(df):\n",
    "    df = pd.get_dummies(df, drop_first=True, dtype=int)\n",
    "    return df\n",
    "\n",
    "# Step 5: Drop low-variance and ID-like columns\n",
    "def drop_low_variance_and_id_columns(df, threshold=0.95):\n",
    "    df = df.copy()\n",
    "    drop_cols = []\n",
    "\n",
    "    for col in df.columns:\n",
    "        if df[col].nunique() <= 1:\n",
    "            drop_cols.append(col)\n",
    "        else:\n",
    "            top_freq_ratio = df[col].value_counts(normalize=True).values[0]\n",
    "            if top_freq_ratio >= threshold:\n",
    "                drop_cols.append(col)\n",
    "\n",
    "    id_like_cols = [col for col in df.columns if col.lower() == 'id'\n",
    "                    or col.lower().startswith('id')\n",
    "                    or col.lower().endswith('id')\n",
    "                    or '_id' in col.lower()\n",
    "                    or 'id_' in col.lower()]\n",
    "    \n",
    "    drop_cols = list(set(drop_cols + id_like_cols))\n",
    "\n",
    "    if drop_cols:\n",
    "        print(f\"üßπ Dropping low-variance/id-like columns: {drop_cols}\")\n",
    "        df.drop(columns=drop_cols, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Step 6: Run all preprocessing\n",
    "def preprocess_data(df):\n",
    "    df = drop_low_variance_and_id_columns(df, threshold=0.95)\n",
    "    df = handle_missing_values(df)\n",
    "    df = handle_outliers(df)\n",
    "    df = create_dummies(df)\n",
    "    return df\n",
    "\n",
    "# MAIN execution inside SageMaker\n",
    "def main():\n",
    "    bucket_name = 'manas-bucket100'\n",
    "    file_key = 'input-file/loan_fraud_data.csv'  # full path in S3 bucket\n",
    "\n",
    "    df = load_csv_from_s3(bucket_name, file_key)\n",
    "    print(f\"üì• Original shape: {df.shape}\")\n",
    "\n",
    "    df = preprocess_data(df)\n",
    "    print(f\"üìä Processed shape: {df.shape}\")\n",
    "\n",
    "    # Save to local file or upload to S3 if needed\n",
    "    processed_file = \"processed_test_data.csv\"\n",
    "    df.to_csv(processed_file, index=False)\n",
    "    print(f\"‚úÖ Preprocessed test data saved: {processed_file}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "# Run if in script mode\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e82fc6c9-ca16-49b3-8bc8-80ba36f37a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: {\"predictions\":[0]}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import json\n",
    "\n",
    "# Create SageMaker runtime client\n",
    "client = boto3.client('sagemaker-runtime', region_name='us-east-1')  # update region if needed\n",
    "\n",
    "# Sample input ‚Äî should match model's expected feature shape and order\n",
    "payload = {\n",
    "    \"inputs\": [[27.0, 1.0, 500987.0, 820870.0, 2.0, 2.0, 0.0, 0.0, 0.0, 0.0]]\n",
    "}\n",
    "\n",
    "# Invoke the deployed endpoint\n",
    "response = client.invoke_endpoint(\n",
    "    EndpointName='custom-ml-endpoint',  # Make sure this matches exactly\n",
    "    Body=json.dumps(payload),\n",
    "    ContentType='application/json'\n",
    ")\n",
    "\n",
    "# Read and decode the prediction\n",
    "result = response['Body'].read().decode('utf-8')\n",
    "print(\"Prediction:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f1358f94-0819-47df-9d26-9653130d0fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Load your CSV\n",
    "df = pd.read_csv('processed_test_data.csv')  # Each row should have feature values\n",
    "\n",
    "# Convert the data to list of lists (assuming features are in columns)\n",
    "data = df.values.tolist()\n",
    "\n",
    "# Initialize SageMaker client\n",
    "client = boto3.client('sagemaker-runtime')\n",
    "\n",
    "# Predict in a loop (or you can do it in batches)\n",
    "predictions = []\n",
    "for row in data:\n",
    "    payload = json.dumps({'inputs': [row]})\n",
    "    response = client.invoke_endpoint(\n",
    "        EndpointName='custom-ml-endpoint',\n",
    "        Body=payload,\n",
    "        ContentType='application/json'\n",
    "    )\n",
    "    result = json.loads(response['Body'].read().decode()) \n",
    "    predictions.append(result)\n",
    "\n",
    "# Save predictions to CSV\n",
    "df['prediction_fraudulent'] = predictions\n",
    "df.to_csv('predictions_fraudulent.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5404743-fa70-43de-aba2-b5654dade0fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Inference logs saved locally as 'inference_logs.csv'\n",
      "‚úÖ Log file uploaded to s3://manas-bucket100/logs/inference_logs_2025-07-27_10-03-12.csv\n"
     ]
    }
   ],
   "source": [
    "####### Fetching the inference log from Sagemaker endpoint and saving it in S3 and locally########\n",
    "##################################################################################################\n",
    "import boto3\n",
    "import pandas as pd \n",
    "import json\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "\n",
    "# --- Config ---\n",
    "endpoint_name = 'custom-ml-endpoint'\n",
    "s3_bucket = 'manas-bucket100'\n",
    "s3_key = f'logs/inference_logs_{datetime.utcnow().strftime(\"%Y-%m-%d_%H-%M-%S\")}.csv'\n",
    "\n",
    "# --- Load data ---\n",
    "df = pd.read_csv('processed_test_data.csv')  # Each row should have feature values\n",
    "data = df.values.tolist()\n",
    "columns = df.columns.tolist()\n",
    "\n",
    "# --- Initialize clients ---\n",
    "client = boto3.client('sagemaker-runtime')\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# --- Prediction + Logging ---\n",
    "log_rows = []\n",
    "predictions = []\n",
    "\n",
    "for row in data:\n",
    "    payload = json.dumps({'inputs': [row]})\n",
    "    try:\n",
    "        response = client.invoke_endpoint(\n",
    "            EndpointName=endpoint_name,\n",
    "            Body=payload,\n",
    "            ContentType='application/json'\n",
    "        )\n",
    "        result = json.loads(response['Body'].read().decode())\n",
    "        prediction = result[\"predictions\"][0] if \"predictions\" in result else result\n",
    "    except Exception as e:\n",
    "        prediction = None\n",
    "        print(f\"Prediction error: {e}\")\n",
    "\n",
    "    log_entry = {\n",
    "        'uuid': str(uuid.uuid4()),\n",
    "        'timestamp': datetime.utcnow().isoformat(),\n",
    "        **{f: v for f, v in zip(columns, row)},\n",
    "        'prediction': prediction\n",
    "    }\n",
    "\n",
    "    log_rows.append(log_entry)\n",
    "    predictions.append(prediction)\n",
    "\n",
    "# --- Save to CSV ---\n",
    "log_df = pd.DataFrame(log_rows)\n",
    "log_df.to_csv('inference_logs.csv', index=False)\n",
    "print(\"‚úÖ Inference logs saved locally as 'inference_logs.csv'\")\n",
    "\n",
    "# --- Upload to S3 ---\n",
    "with open(\"inference_logs.csv\", \"rb\") as f:\n",
    "    s3.upload_fileobj(f, Bucket=s3_bucket, Key=s3_key)\n",
    "    print(f\"‚úÖ Log file uploaded to s3://{s3_bucket}/{s3_key}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c294f004-2326-4e14-85b7-9bac47c80322",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Drift detection report saved as 'drift_detection_report.csv'\n"
     ]
    }
   ],
   "source": [
    "###### Drift Detections ########\n",
    "################################\n",
    "import pandas as pd\n",
    "from scipy.stats import ks_2samp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Load Data ---\n",
    "reference_df = pd.read_csv(\"processed_test_data.csv\")  # Your stored baseline (used for training)\n",
    "inference_df = pd.read_csv(\"inference_logs.csv\")  # Today's logged inputs\n",
    "\n",
    "# --- Feature Columns (exclude prediction column) ---\n",
    "feature_columns = [col for col in reference_df.columns if col in inference_df.columns and col not in ['prediction', 'timestamp', 'uuid']]\n",
    "\n",
    "# --- Run KS-Test and Plot ---\n",
    "drift_results = []\n",
    "\n",
    "for col in feature_columns:\n",
    "    ref_values = reference_df[col].dropna()\n",
    "    inf_values = inference_df[col].dropna()\n",
    "\n",
    "    # Kolmogorov-Smirnov test\n",
    "    statistic, p_value = ks_2samp(ref_values, inf_values)\n",
    "\n",
    "    drift_results.append({\n",
    "        'feature': col,\n",
    "        'ks_statistic': statistic,\n",
    "        'p_value': p_value,\n",
    "        'drift_detected': p_value < 0.05\n",
    "    })\n",
    "\n",
    "    # Optional: Plot distribution\n",
    "    plt.figure(figsize=(6, 3))\n",
    "    plt.hist(ref_values, bins=30, alpha=0.5, label='Reference')\n",
    "    plt.hist(inf_values, bins=30, alpha=0.5, label='Inference')\n",
    "    plt.title(f'Distribution of {col}')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'drift_plot_{col}.png')\n",
    "    plt.close()\n",
    "\n",
    "# --- Save Results ---\n",
    "drift_df = pd.DataFrame(drift_results)\n",
    "drift_df.to_csv('drift_detection_report.csv', index=False)\n",
    "print(\"‚úÖ Drift detection report saved as 'drift_detection_report.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0941ce-f9a8-4c8b-9697-51becc854993",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
