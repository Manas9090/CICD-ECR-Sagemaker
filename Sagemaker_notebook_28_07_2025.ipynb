{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5e6835-ef2e-4970-a833-f58bd4f5e91d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----!"
     ]
    }
   ],
   "source": [
    "###### Model Deployment in Sagemaker #########\n",
    "##############################################\n",
    "import boto3\n",
    "from sagemaker.model import Model\n",
    "from sagemaker import get_execution_role, Session \n",
    "\n",
    "role = get_execution_role()\n",
    "session = Session()\n",
    "\n",
    "ecr_image = '491085388405.dkr.ecr.us-east-1.amazonaws.com/fraud-ecr-28:latest'\n",
    "\n",
    "model = Model(\n",
    "    image_uri=ecr_image,\n",
    "    role=role,\n",
    "    sagemaker_session=session\n",
    ")\n",
    "\n",
    "predictor = model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.m5.large',\n",
    "    endpoint_name='fraud-ml-endpoint'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4547a724-bfb9-49d0-9d0a-f95191b8d412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: {\"predictions\":[0]}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "######## Inference using one row of Sample Data #########\n",
    "#########################################################\n",
    "import boto3\n",
    "import json\n",
    "\n",
    "# Create SageMaker runtime client\n",
    "client = boto3.client('sagemaker-runtime', region_name='us-east-1')  # update region if needed\n",
    "\n",
    "# Sample input — should match model's expected feature shape and order\n",
    "payload = {\n",
    "    \"inputs\": [[27.0, 1.0, 500987.0, 820870.0, 2.0, 2.0, 0.0, 0.0, 0.0, 0.0]]\n",
    "}\n",
    "\n",
    "# Invoke the deployed endpoint\n",
    "response = client.invoke_endpoint(\n",
    "    EndpointName='fraud-ml-endpoint',  # Make sure this matches exactly\n",
    "    Body=json.dumps(payload),\n",
    "    ContentType='application/json'\n",
    ")\n",
    "\n",
    "# Read and decode the prediction\n",
    "result = response['Body'].read().decode('utf-8')\n",
    "print(\"Prediction:\", result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27be0cc2-db8b-41a2-a358-55e0395140c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: psycopg2-binary in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (2.9.9)\n",
      "Requirement already satisfied: boto3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (1.38.38)\n",
      "Requirement already satisfied: sqlalchemy in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (2.0.41)\n",
      "Requirement already satisfied: psycopg2==2.9.9 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from psycopg2-binary) (2.9.9)\n",
      "Requirement already satisfied: botocore<1.39.0,>=1.38.38 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from boto3) (1.38.38)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from boto3) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.14.0,>=0.13.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from boto3) (0.13.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from botocore<1.39.0,>=1.38.38->boto3) (2.9.0.post0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from botocore<1.39.0,>=1.38.38->boto3) (2.4.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.39.0,>=1.38.38->boto3) (1.17.0)\n",
      "Requirement already satisfied: greenlet>=1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sqlalchemy) (3.2.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sqlalchemy) (4.14.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install psycopg2-binary boto3 sqlalchemy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5118695e-43c6-4bea-a020-4649a5052934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting supabase\n",
      "  Downloading supabase-2.17.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting gotrue==2.12.3 (from supabase)\n",
      "  Downloading gotrue-2.12.3-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting httpx<0.29,>=0.26 (from supabase)\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting postgrest==1.1.1 (from supabase)\n",
      "  Downloading postgrest-1.1.1-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting realtime==2.6.0 (from supabase)\n",
      "  Downloading realtime-2.6.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting storage3==0.12.0 (from supabase)\n",
      "  Downloading storage3-0.12.0-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting supafunc==0.10.1 (from supabase)\n",
      "  Downloading supafunc-0.10.1-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: pydantic<3,>=1.10 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gotrue==2.12.3->supabase) (2.9.2)\n",
      "Requirement already satisfied: pyjwt<3.0.0,>=2.10.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gotrue==2.12.3->supabase) (2.10.1)\n",
      "Collecting deprecation<3.0.0,>=2.1.0 (from postgrest==1.1.1->supabase)\n",
      "  Downloading deprecation-2.1.0-py2.py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting strenum<0.5.0,>=0.4.9 (from postgrest==1.1.1->supabase)\n",
      "  Downloading StrEnum-0.4.15-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting pydantic<3,>=1.10 (from gotrue==2.12.3->supabase)\n",
      "  Using cached pydantic-2.11.7-py3-none-any.whl.metadata (67 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.14.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from realtime==2.6.0->supabase) (4.14.0)\n",
      "Collecting websockets<16,>=11 (from realtime==2.6.0->supabase)\n",
      "  Downloading websockets-15.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.8.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from storage3==0.12.0->supabase) (2.9.0.post0)\n",
      "Requirement already satisfied: packaging in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from deprecation<3.0.0,>=2.1.0->postgrest==1.1.1->supabase) (24.2)\n",
      "Requirement already satisfied: anyio in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from httpx<0.29,>=0.26->supabase) (4.9.0)\n",
      "Requirement already satisfied: certifi in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from httpx<0.29,>=0.26->supabase) (2025.6.15)\n",
      "Collecting httpcore==1.* (from httpx<0.29,>=0.26->supabase)\n",
      "  Using cached httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: idna in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from httpx<0.29,>=0.26->supabase) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from httpcore==1.*->httpx<0.29,>=0.26->supabase) (0.16.0)\n",
      "Requirement already satisfied: h2<5,>=3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from httpx[http2]<0.29,>=0.26->gotrue==2.12.3->supabase) (4.2.0)\n",
      "Requirement already satisfied: hyperframe<7,>=6.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from h2<5,>=3->httpx[http2]<0.29,>=0.26->gotrue==2.12.3->supabase) (6.1.0)\n",
      "Requirement already satisfied: hpack<5,>=4.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from h2<5,>=3->httpx[http2]<0.29,>=0.26->gotrue==2.12.3->supabase) (4.1.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pydantic<3,>=1.10->gotrue==2.12.3->supabase) (0.7.0)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic<3,>=1.10->gotrue==2.12.3->supabase)\n",
      "  Using cached pydantic_core-2.33.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic<3,>=1.10->gotrue==2.12.3->supabase)\n",
      "  Using cached typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.8.2->storage3==0.12.0->supabase) (1.17.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from anyio->httpx<0.29,>=0.26->supabase) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from anyio->httpx<0.29,>=0.26->supabase) (1.3.1)\n",
      "Downloading supabase-2.17.0-py3-none-any.whl (17 kB)\n",
      "Downloading gotrue-2.12.3-py3-none-any.whl (44 kB)\n",
      "Downloading postgrest-1.1.1-py3-none-any.whl (22 kB)\n",
      "Downloading realtime-2.6.0-py3-none-any.whl (21 kB)\n",
      "Downloading storage3-0.12.0-py3-none-any.whl (18 kB)\n",
      "Downloading supafunc-0.10.1-py3-none-any.whl (8.0 kB)\n",
      "Downloading deprecation-2.1.0-py2.py3-none-any.whl (11 kB)\n",
      "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Using cached httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Using cached pydantic-2.11.7-py3-none-any.whl (444 kB)\n",
      "Using cached pydantic_core-2.33.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "Downloading StrEnum-0.4.15-py3-none-any.whl (8.9 kB)\n",
      "Downloading websockets-15.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (181 kB)\n",
      "Using cached typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: strenum, websockets, typing-inspection, pydantic-core, httpcore, deprecation, pydantic, realtime, httpx, supafunc, storage3, postgrest, gotrue, supabase\n",
      "\u001b[2K  Attempting uninstall: pydantic-core\n",
      "\u001b[2K    Found existing installation: pydantic_core 2.23.4\n",
      "\u001b[2K    Uninstalling pydantic_core-2.23.4:\n",
      "\u001b[2K      Successfully uninstalled pydantic_core-2.23.4\n",
      "\u001b[2K  Attempting uninstall: pydantic90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/14\u001b[0m [pydantic-core]\n",
      "\u001b[2K    Found existing installation: pydantic 2.9.2━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/14\u001b[0m [pydantic-core]\n",
      "\u001b[2K    Uninstalling pydantic-2.9.2:0m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 6/14\u001b[0m [pydantic]]\n",
      "\u001b[2K      Successfully uninstalled pydantic-2.9.2━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 6/14\u001b[0m [pydantic]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14/14\u001b[0m [supabase]/14\u001b[0m [postgrest]\n",
      "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "safety-schemas 0.0.14 requires pydantic<2.10.0,>=2.6.0, but you have pydantic 2.11.7 which is incompatible.\n",
      "sparkmagic 0.21.0 requires pandas<2.0.0,>=0.17.1, but you have pandas 2.2.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed deprecation-2.1.0 gotrue-2.12.3 httpcore-1.0.9 httpx-0.28.1 postgrest-1.1.1 pydantic-2.11.7 pydantic-core-2.33.2 realtime-2.6.0 storage3-0.12.0 strenum-0.4.15 supabase-2.17.0 supafunc-0.10.1 typing-inspection-0.4.1 websockets-15.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install supabase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44b54da5-f6ca-446f-9107-8dbb82356cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Upload to S3 complete\n"
     ]
    }
   ],
   "source": [
    "###### Fetching the Data from Supabase and storing in S3 ########\n",
    "#################################################################\n",
    "# print(\"Supabase raw response:\", response)\n",
    "# print(\"Supabase data:\", response.data)\n",
    "\n",
    "\n",
    "# print(\"DataFrame shape:\", df.shape)\n",
    "# print(\"DataFrame head:\\n\", df.head())\n",
    "\n",
    "\n",
    "from supabase import create_client, Client\n",
    "import pandas as pd\n",
    "import boto3\n",
    "\n",
    "# Supabase config\n",
    "SUPABASE_URL = \"https://mcgzvjzuqnjstptfrsuj.supabase.co\"\n",
    "SUPABASE_KEY = \"sb_publishable_4vx6B3HjwnEQVwI9SgrJkQ_S32tu5l5\"\n",
    "\n",
    "# Use the service_role key, not anon\n",
    "\n",
    "# Create Supabase client\n",
    "supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)\n",
    "\n",
    "# Fetch data\n",
    "response = supabase.table(\"loan_fraud_analytics\").select(\"*\").execute()\n",
    "df = pd.DataFrame(response.data)\n",
    "\n",
    "# Save as CSV\n",
    "csv_path = \"/tmp/loan_fraud_data.csv\"\n",
    "df.to_csv(csv_path, index=False)\n",
    "\n",
    "# Upload to S3\n",
    "s3 = boto3.client(\"s3\")\n",
    "bucket_name = \"manas-bucket100\"\n",
    "object_key = \"inputfile/loan_fraud_data.csv\"\n",
    "\n",
    "s3.upload_file(csv_path, bucket_name, object_key)\n",
    "print(\"✅ Upload to S3 complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3be33925-e614-464c-b680-1f03c6b7ad7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Features uploaded to s3://manas-bucket100/inputfile/loan_fraud_data.csv\n"
     ]
    }
   ],
   "source": [
    "######## Dropping the target variable ########\n",
    "##############################################\n",
    "import boto3\n",
    "import io\n",
    "\n",
    "# Drop target column\n",
    "df_features = df.drop(columns=['loan_default'])\n",
    "\n",
    "# Convert to CSV buffer\n",
    "csv_buffer = io.StringIO()\n",
    "df_features.to_csv(csv_buffer, index=False)\n",
    "\n",
    "# Upload to S3\n",
    "s3 = boto3.client('s3')\n",
    "bucket_name = 'manas-bucket100'  # 🔁 Your bucket name\n",
    "object_key = 'inputfile/loan_fraud_data.csv'\n",
    "\n",
    "s3.put_object(Bucket=bucket_name, Key=object_key, Body=csv_buffer.getvalue())\n",
    "\n",
    "print(f\"✅ Features uploaded to s3://{bucket_name}/{object_key}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65be84ef-c2e9-4627-970b-9ea48453e1b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded data from s3://manas-bucket100/inputfile/loan_fraud_data.csv\n",
      "📥 Original shape: (1000, 9)\n",
      "🧹 Dropping low-variance/id-like columns: ['loan_id', 'last_delinq_none']\n",
      "📊 Processed shape: (1000, 10)\n",
      "✅ Preprocessed test data saved: processed_test_data.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8687/2994902416.py:26: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(df[col].median(), inplace=True)\n",
      "/tmp/ipykernel_8687/2994902416.py:24: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(df[col].mode()[0], inplace=True)\n"
     ]
    }
   ],
   "source": [
    "####### Code for Data processing ########\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import boto3\n",
    "from io import StringIO\n",
    "bucket_name = \"manas-bucket100\"\n",
    "s3_key = \"inputfile/loan_fraud_data.csv\"\n",
    "\n",
    "# Step 1: Define S3 download\n",
    "def load_csv_from_s3(bucket_name, file_key):\n",
    "    s3 = boto3.client('s3')\n",
    "    response = s3.get_object(Bucket=bucket_name, Key=file_key)\n",
    "    content = response['Body'].read().decode('utf-8')\n",
    "    df = pd.read_csv(StringIO(content))\n",
    "    print(f\"✅ Loaded data from s3://{bucket_name}/{file_key}\")\n",
    "    return df\n",
    "\n",
    "# Step 2: Handle missing values\n",
    "def handle_missing_values(df):\n",
    "    df = df.copy()\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'object':\n",
    "            df[col].fillna(df[col].mode()[0], inplace=True)\n",
    "        else:\n",
    "            df[col].fillna(df[col].median(), inplace=True)\n",
    "    return df\n",
    "\n",
    "# Step 3: Handle outliers\n",
    "def handle_outliers(df):\n",
    "    df = df.copy()\n",
    "    for col in df.select_dtypes(include=np.number).columns:\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower = Q1 - 1.5 * IQR\n",
    "        upper = Q3 + 1.5 * IQR\n",
    "        df[col] = np.where(df[col] < lower, lower,\n",
    "                  np.where(df[col] > upper, upper, df[col]))\n",
    "    return df\n",
    "\n",
    "# Step 4: Create dummies\n",
    "def create_dummies(df):\n",
    "    df = pd.get_dummies(df, drop_first=True, dtype=int)\n",
    "    return df\n",
    "\n",
    "# Step 5: Drop low-variance and ID-like columns\n",
    "def drop_low_variance_and_id_columns(df, threshold=0.95):\n",
    "    df = df.copy()\n",
    "    drop_cols = []\n",
    "\n",
    "    for col in df.columns:\n",
    "        if df[col].nunique() <= 1:\n",
    "            drop_cols.append(col)\n",
    "        else:\n",
    "            top_freq_ratio = df[col].value_counts(normalize=True).values[0]\n",
    "            if top_freq_ratio >= threshold:\n",
    "                drop_cols.append(col)\n",
    "\n",
    "    id_like_cols = [col for col in df.columns if col.lower() == 'id'\n",
    "                    or col.lower().startswith('id')\n",
    "                    or col.lower().endswith('id')\n",
    "                    or '_id' in col.lower()\n",
    "                    or 'id_' in col.lower()]\n",
    "    \n",
    "    drop_cols = list(set(drop_cols + id_like_cols))\n",
    "\n",
    "    if drop_cols:\n",
    "        print(f\"🧹 Dropping low-variance/id-like columns: {drop_cols}\")\n",
    "        df.drop(columns=drop_cols, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Step 6: Run all preprocessing\n",
    "def preprocess_data(df):\n",
    "    df = drop_low_variance_and_id_columns(df, threshold=0.95)\n",
    "    df = handle_missing_values(df)\n",
    "    df = handle_outliers(df)\n",
    "    df = create_dummies(df)\n",
    "    return df\n",
    "\n",
    "# MAIN execution inside SageMaker\n",
    "def main():\n",
    "    bucket_name = 'manas-bucket100'\n",
    "    file_key = 'inputfile/loan_fraud_data.csv'  # full path in S3 bucket\n",
    "\n",
    "    df = load_csv_from_s3(bucket_name, file_key)\n",
    "    print(f\"📥 Original shape: {df.shape}\")\n",
    "\n",
    "    df = preprocess_data(df)\n",
    "    print(f\"📊 Processed shape: {df.shape}\")\n",
    "\n",
    "    # Save to local file or upload to S3 if needed\n",
    "    processed_file = \"processed_test_data.csv\"\n",
    "    df.to_csv(processed_file, index=False)\n",
    "    print(f\"✅ Preprocessed test data saved: {processed_file}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "# Run if in script mode\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9aeae457-8a90-4a6e-b499-5baba3bcc5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "######### Prediction with Processed bulk Data #########\n",
    "#######################################################\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Load your CSV\n",
    "df = pd.read_csv('processed_test_data.csv')  # Each row should have feature values\n",
    "\n",
    "# Convert the data to list of lists (assuming features are in columns)\n",
    "data = df.values.tolist()\n",
    "\n",
    "# Initialize SageMaker client\n",
    "client = boto3.client('sagemaker-runtime')\n",
    "\n",
    "# Predict in a loop (or you can do it in batches)\n",
    "predictions = []\n",
    "for row in data:\n",
    "    payload = json.dumps({'inputs': [row]})\n",
    "    response = client.invoke_endpoint(\n",
    "        EndpointName='fraud-ml-endpoint',\n",
    "        Body=payload,\n",
    "        ContentType='application/json'\n",
    "    )\n",
    "    result = json.loads(response['Body'].read().decode()) \n",
    "    predictions.append(result)\n",
    "\n",
    "# Save predictions to CSV\n",
    "df['prediction_fraudulent'] = predictions\n",
    "df.to_csv('predictions_fraudulent.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22fd6590-4d83-4f5d-a83a-1f71a19799eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Inference logs saved locally as 'inference_logs.csv'\n",
      "✅ Log file uploaded to s3://manas-bucket100/logs/inference_logs_2025-07-28_08-07-58.csv\n"
     ]
    }
   ],
   "source": [
    "####### Fetching the inference log from Sagemaker endpoint and saving it in S3 and locally ########\n",
    "###################################################################################################\n",
    "import boto3\n",
    "import pandas as pd \n",
    "import json\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "\n",
    "# --- Config ---\n",
    "endpoint_name = 'fraud-ml-endpoint'\n",
    "s3_bucket = 'manas-bucket100'\n",
    "s3_key = f'logs/inference_logs_{datetime.utcnow().strftime(\"%Y-%m-%d_%H-%M-%S\")}.csv'\n",
    "\n",
    "# --- Load data ---\n",
    "df = pd.read_csv('processed_test_data.csv')  # Each row should have feature values\n",
    "data = df.values.tolist()\n",
    "columns = df.columns.tolist()\n",
    "\n",
    "# --- Initialize clients ---\n",
    "client = boto3.client('sagemaker-runtime')\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# --- Prediction + Logging ---\n",
    "log_rows = []\n",
    "predictions = []\n",
    "\n",
    "for row in data:\n",
    "    payload = json.dumps({'inputs': [row]})\n",
    "    try:\n",
    "        response = client.invoke_endpoint(\n",
    "            EndpointName=endpoint_name,\n",
    "            Body=payload,\n",
    "            ContentType='application/json'\n",
    "        )\n",
    "        result = json.loads(response['Body'].read().decode())\n",
    "        prediction = result[\"predictions\"][0] if \"predictions\" in result else result\n",
    "    except Exception as e:\n",
    "        prediction = None\n",
    "        print(f\"Prediction error: {e}\")\n",
    "\n",
    "    log_entry = {\n",
    "        'uuid': str(uuid.uuid4()),\n",
    "        'timestamp': datetime.utcnow().isoformat(),\n",
    "        **{f: v for f, v in zip(columns, row)},\n",
    "        'prediction': prediction\n",
    "    }\n",
    "\n",
    "    log_rows.append(log_entry)\n",
    "    predictions.append(prediction)\n",
    "\n",
    "# --- Save to CSV ---\n",
    "log_df = pd.DataFrame(log_rows)\n",
    "log_df.to_csv('inference_logs.csv', index=False)\n",
    "print(\"✅ Inference logs saved locally as 'inference_logs.csv'\")\n",
    "\n",
    "# --- Upload to S3 ---\n",
    "with open(\"inference_logs.csv\", \"rb\") as f:\n",
    "    s3.upload_fileobj(f, Bucket=s3_bucket, Key=s3_key)\n",
    "    print(f\"✅ Log file uploaded to s3://{s3_bucket}/{s3_key}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "60dc4331-b464-4d86-8bd9-50e5bf9a3577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Drift detection complete.\n",
      "📄 Report: drift_detection_report.csv\n",
      "📊 Plots: saved in 'drift_plots/' folder\n"
     ]
    }
   ],
   "source": [
    "#######  Drift detection Analysis ##### Drift analysis and charts ######\n",
    "########################################################################\n",
    "import pandas as pd\n",
    "from scipy.stats import ks_2samp\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# --- Load Data ---\n",
    "reference_df = pd.read_csv(\"processed_test_data.csv\")\n",
    "inference_df = pd.read_csv(\"inference_logs.csv\")\n",
    "\n",
    "# --- Feature Columns ---\n",
    "feature_columns = [col for col in reference_df.columns if col in inference_df.columns and col not in ['prediction', 'timestamp', 'uuid']]\n",
    "\n",
    "# --- Create output folder ---\n",
    "os.makedirs(\"drift_plots\", exist_ok=True)\n",
    "\n",
    "# --- Drift Detection ---\n",
    "drift_results = []\n",
    "\n",
    "for col in feature_columns:\n",
    "    try:\n",
    "        ref_values = reference_df[col].dropna()\n",
    "        inf_values = inference_df[col].dropna()\n",
    "\n",
    "        # Ensure numeric and non-empty\n",
    "        if ref_values.empty or inf_values.empty or not np.issubdtype(ref_values.dtype, np.number) or not np.issubdtype(inf_values.dtype, np.number):\n",
    "            print(f\"⚠️ Skipping '{col}' (non-numeric or empty)\")\n",
    "            continue\n",
    "\n",
    "        # KS-Test\n",
    "        statistic, p_value = ks_2samp(ref_values, inf_values)\n",
    "        drift_results.append({\n",
    "            'feature': col,\n",
    "            'ks_statistic': statistic,\n",
    "            'p_value': p_value,\n",
    "            'drift_detected': p_value < 0.05\n",
    "        })\n",
    "\n",
    "        # Plot (side-by-side, independent y-scale)\n",
    "        fig, axs = plt.subplots(1, 2, figsize=(10, 4), sharey=False)\n",
    "\n",
    "        axs[0].hist(ref_values, bins=30, color='skyblue', edgecolor='black')\n",
    "        axs[0].set_title(f'Reference: {col}')\n",
    "        axs[0].set_xlabel(col)\n",
    "        axs[0].set_ylabel('Frequency')\n",
    "\n",
    "        axs[1].hist(inf_values, bins=30, color='salmon', edgecolor='black')\n",
    "        axs[1].set_title(f'Inference: {col}')\n",
    "        axs[1].set_xlabel(col)\n",
    "        axs[1].set_ylabel('Frequency')\n",
    "\n",
    "        plt.suptitle(f'Distribution Comparison - {col}')\n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "        plt.savefig(f'drift_plots/drift_plot_{col}.png')\n",
    "        plt.close()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error processing feature '{col}': {str(e)}\")\n",
    "\n",
    "# --- Save Report ---\n",
    "drift_df = pd.DataFrame(drift_results)\n",
    "drift_df.to_csv('drift_detection_report.csv', index=False)\n",
    "\n",
    "print(\"\\n✅ Drift detection complete.\")\n",
    "print(\"📄 Report: drift_detection_report.csv\")\n",
    "print(\"📊 Plots: saved in 'drift_plots/' folder\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "677205ef-4471-4a08-9a6e-44fd78ed4901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Drift report and plots uploaded to S3\n",
      "📧 Email sent: ✅ No Drift Detected\n"
     ]
    }
   ],
   "source": [
    "#######  Drift detection Analysis ##### Drift analysis and charts ###### Saving Report in S3 ##### \n",
    "############################## Email Notification ##########################################\n",
    "import pandas as pd\n",
    "from scipy.stats import ks_2samp\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import boto3\n",
    "import smtplib\n",
    "from email.mime.text import MIMEText\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "from email.mime.application import MIMEApplication\n",
    "\n",
    "# --- Configuration ---\n",
    "BUCKET_NAME = \"manas-bucket100\"\n",
    "DRIFT_FOLDER = \"drift_reports/\"\n",
    "EMAIL_SENDER = \"mmohanty335@gmail.com\"\n",
    "EMAIL_RECEIVER = \"mother.manas15@gmail.com\"\n",
    "EMAIL_PASSWORD = \"dawl cfoq onpw lpec\"\n",
    "SMTP_SERVER = \"smtp.gmail.com\"\n",
    "SMTP_PORT = 587\n",
    "\n",
    "# --- Load Data ---\n",
    "reference_df = pd.read_csv(\"processed_test_data.csv\")\n",
    "inference_df = pd.read_csv(\"inference_logs.csv\")\n",
    "\n",
    "# --- Feature Columns ---\n",
    "feature_columns = [col for col in reference_df.columns if col in inference_df.columns and col not in ['prediction', 'timestamp', 'uuid']]\n",
    "\n",
    "# --- Create output folder ---\n",
    "os.makedirs(\"drift_plots\", exist_ok=True)\n",
    "\n",
    "# --- Drift Detection ---\n",
    "drift_results = []\n",
    "\n",
    "for col in feature_columns:\n",
    "    try:\n",
    "        ref_values = reference_df[col].dropna()\n",
    "        inf_values = inference_df[col].dropna()\n",
    "\n",
    "        if ref_values.empty or inf_values.empty or not np.issubdtype(ref_values.dtype, np.number) or not np.issubdtype(inf_values.dtype, np.number):\n",
    "            continue\n",
    "\n",
    "        statistic, p_value = ks_2samp(ref_values, inf_values)\n",
    "        drift_results.append({\n",
    "            'feature': col,\n",
    "            'ks_statistic': statistic,\n",
    "            'p_value': p_value,\n",
    "            'drift_detected': p_value < 0.05\n",
    "        })\n",
    "\n",
    "        fig, axs = plt.subplots(1, 2, figsize=(10, 4), sharey=False)\n",
    "\n",
    "        axs[0].hist(ref_values, bins=30, color='skyblue', edgecolor='black')\n",
    "        axs[0].set_title(f'Reference: {col}')\n",
    "        axs[0].set_xlabel(col)\n",
    "        axs[0].set_ylabel('Frequency')\n",
    "\n",
    "        axs[1].hist(inf_values, bins=30, color='salmon', edgecolor='black')\n",
    "        axs[1].set_title(f'Inference: {col}')\n",
    "        axs[1].set_xlabel(col)\n",
    "        axs[1].set_ylabel('Frequency')\n",
    "\n",
    "        plt.suptitle(f'Distribution Comparison - {col}')\n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "        plot_path = f'drift_plots/drift_plot_{col}.png'\n",
    "        plt.savefig(plot_path)\n",
    "        plt.close()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error on {col}: {str(e)}\")\n",
    "\n",
    "# --- Save Report Locally ---\n",
    "drift_df = pd.DataFrame(drift_results)\n",
    "report_path = \"drift_detection_report.csv\"\n",
    "drift_df.to_csv(report_path, index=False)\n",
    "\n",
    "# --- Upload to S3 ---\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# Upload report\n",
    "s3.upload_file(report_path, BUCKET_NAME, f\"{DRIFT_FOLDER}drift_detection_report.csv\")\n",
    "\n",
    "# Upload plots\n",
    "for file in os.listdir(\"drift_plots\"):\n",
    "    if file.endswith(\".png\"):\n",
    "        s3.upload_file(f\"drift_plots/{file}\", BUCKET_NAME, f\"{DRIFT_FOLDER}plots/{file}\")\n",
    "\n",
    "print(\"✅ Drift report and plots uploaded to S3\")\n",
    "\n",
    "# --- Email Notification ---\n",
    "def send_email(subject, body):\n",
    "    msg = MIMEMultipart()\n",
    "    msg['From'] = EMAIL_SENDER\n",
    "    msg['To'] = EMAIL_RECEIVER\n",
    "    msg['Subject'] = subject\n",
    "\n",
    "    msg.attach(MIMEText(body, 'plain'))\n",
    "\n",
    "    with smtplib.SMTP(SMTP_SERVER, SMTP_PORT) as server:\n",
    "        server.starttls()\n",
    "        server.login(EMAIL_SENDER, EMAIL_PASSWORD)\n",
    "        server.send_message(msg)\n",
    "\n",
    "# Determine message\n",
    "if drift_df['drift_detected'].any():\n",
    "    subject = \"⚠️ Drift Detected in Model Monitoring\"\n",
    "    body = \"⚠️ Attention - Drift detected. Please return to Action mode!\"\n",
    "else:\n",
    "    subject = \"✅ No Drift Detected\"\n",
    "    body = \"✅ No drift detected. Just chill 😎.\"\n",
    "\n",
    "send_email(subject, body)\n",
    "print(f\"📧 Email sent: {subject}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4039f1d9-719e-4b2f-86fa-9a4e94ceb625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Drift report and plots uploaded to S3\n",
      "📧 Email sent with drift report attached.\n"
     ]
    }
   ],
   "source": [
    "#######  Drift detection Analysis ##### Drift analysis and charts ###### Saving Report in S3 ##### \n",
    "############################## Email Notification with attached Report ############################\n",
    "\n",
    "import pandas as pd\n",
    "from scipy.stats import ks_2samp\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import boto3\n",
    "import smtplib\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "from email.mime.text import MIMEText\n",
    "from email.mime.base import MIMEBase\n",
    "from email import encoders\n",
    "\n",
    "# --------------------\n",
    "# CONFIGURATION\n",
    "# --------------------\n",
    "# AWS S3\n",
    "S3_BUCKET_NAME = 'manas-bucket100'\n",
    "S3_REPORT_PATH = 'drift_reports/drift_detection_report.csv'\n",
    "S3_PLOTS_DIR = 'drift_reports/plots/'\n",
    "\n",
    "# Email\n",
    "EMAIL_SENDER = 'mmohanty335@gmail.com'\n",
    "EMAIL_RECEIVER = 'mother.manas15@gmail.com'\n",
    "EMAIL_PASSWORD = 'dawl cfoq onpw lpec'  # e.g., Gmail App Password\n",
    "SMTP_SERVER = 'smtp.gmail.com'\n",
    "SMTP_PORT = 587\n",
    "\n",
    "# --------------------\n",
    "# LOAD DATA\n",
    "# --------------------\n",
    "reference_df = pd.read_csv(\"processed_test_data.csv\")\n",
    "inference_df = pd.read_csv(\"inference_logs.csv\")\n",
    "feature_columns = [col for col in reference_df.columns if col in inference_df.columns and col not in ['prediction', 'timestamp', 'uuid']]\n",
    "\n",
    "os.makedirs(\"drift_plots\", exist_ok=True)\n",
    "\n",
    "# --------------------\n",
    "# DRIFT DETECTION\n",
    "# --------------------\n",
    "drift_results = []\n",
    "for col in feature_columns:\n",
    "    try:\n",
    "        ref_values = reference_df[col].dropna()\n",
    "        inf_values = inference_df[col].dropna()\n",
    "\n",
    "        if ref_values.empty or inf_values.empty or not np.issubdtype(ref_values.dtype, np.number) or not np.issubdtype(inf_values.dtype, np.number):\n",
    "            print(f\"⚠️ Skipping '{col}' (non-numeric or empty)\")\n",
    "            continue\n",
    "\n",
    "        statistic, p_value = ks_2samp(ref_values, inf_values)\n",
    "        drift_results.append({\n",
    "            'feature': col,\n",
    "            'ks_statistic': statistic,\n",
    "            'p_value': p_value,\n",
    "            'drift_detected': p_value < 0.05\n",
    "        })\n",
    "\n",
    "        fig, axs = plt.subplots(1, 2, figsize=(10, 4), sharey=False)\n",
    "        axs[0].hist(ref_values, bins=30, color='skyblue', edgecolor='black')\n",
    "        axs[0].set_title(f'Reference: {col}')\n",
    "        axs[1].hist(inf_values, bins=30, color='salmon', edgecolor='black')\n",
    "        axs[1].set_title(f'Inference: {col}')\n",
    "        plt.suptitle(f'Distribution Comparison - {col}')\n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "        local_plot_path = f'drift_plots/drift_plot_{col}.png'\n",
    "        plt.savefig(local_plot_path)\n",
    "        plt.close()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error processing feature '{col}': {str(e)}\")\n",
    "\n",
    "# --------------------\n",
    "# SAVE REPORT LOCALLY\n",
    "# --------------------\n",
    "drift_df = pd.DataFrame(drift_results)\n",
    "drift_report_path = 'drift_detection_report.csv'\n",
    "drift_df.to_csv(drift_report_path, index=False)\n",
    "\n",
    "# --------------------\n",
    "# UPLOAD TO S3\n",
    "# --------------------\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# Upload report\n",
    "s3.upload_file(drift_report_path, S3_BUCKET_NAME, S3_REPORT_PATH)\n",
    "\n",
    "# Upload plots\n",
    "for filename in os.listdir(\"drift_plots\"):\n",
    "    if filename.endswith(\".png\"):\n",
    "        local_path = os.path.join(\"drift_plots\", filename)\n",
    "        s3_key = f\"{S3_PLOTS_DIR}{filename}\"\n",
    "        s3.upload_file(local_path, S3_BUCKET_NAME, s3_key)\n",
    "\n",
    "print(\"✅ Drift report and plots uploaded to S3\")\n",
    "\n",
    "# --------------------\n",
    "# SEND EMAIL\n",
    "# --------------------\n",
    "def send_email_with_attachment(subject, body, attachment_path):\n",
    "    msg = MIMEMultipart()\n",
    "    msg['From'] = EMAIL_SENDER\n",
    "    msg['To'] = EMAIL_RECEIVER\n",
    "    msg['Subject'] = subject\n",
    "\n",
    "    msg.attach(MIMEText(body, 'plain'))\n",
    "\n",
    "    # Attach CSV\n",
    "    part = MIMEBase('application', 'octet-stream')\n",
    "    with open(attachment_path, 'rb') as f:\n",
    "        part.set_payload(f.read())\n",
    "    encoders.encode_base64(part)\n",
    "    part.add_header('Content-Disposition', f'attachment; filename={os.path.basename(attachment_path)}')\n",
    "    msg.attach(part)\n",
    "\n",
    "    with smtplib.SMTP(SMTP_SERVER, SMTP_PORT) as server:\n",
    "        server.starttls()\n",
    "        server.login(EMAIL_SENDER, EMAIL_PASSWORD)\n",
    "        server.send_message(msg)\n",
    "\n",
    "# Email logic\n",
    "if drift_df['drift_detected'].any():\n",
    "    subject = \"⚠️ Drift Detected in Model Monitoring\"\n",
    "    body = \"⚠️ Attention — Drift detected. Please return to Action mode!\"\n",
    "else:\n",
    "    subject = \"✅ All Clear - No Drift Detected\"\n",
    "    body = \"🎉 Hooray! No drift detected — just chill 😎\"\n",
    "\n",
    "send_email_with_attachment(subject, body, drift_report_path)\n",
    "print(\"📧 Email sent with drift report attached.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81bc698-f726-4963-ad36-f85458f24ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######  Mixed Drift Detection Method ########\n",
    "##############################################\n",
    "from scipy.stats import ks_2samp, chi2_contingency \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def detect_drift_numerical(ref_series, new_series):\n",
    "    stat, p_value = ks_2samp(ref_series, new_series)\n",
    "    return p_value < 0.05\n",
    "\n",
    "def detect_drift_categorical(ref_series, new_series):\n",
    "    ref_counts = ref_series.value_counts().sort_index()\n",
    "    new_counts = new_series.value_counts().sort_index()\n",
    "    # Align indexes\n",
    "    all_categories = sorted(set(ref_counts.index).union(set(new_counts.index)))\n",
    "    ref_freq = [ref_counts.get(cat, 0) for cat in all_categories]\n",
    "    new_freq = [new_counts.get(cat, 0) for cat in all_categories]\n",
    "    stat, p, _, _ = chi2_contingency([ref_freq, new_freq])\n",
    "    return p < 0.05\n",
    "\n",
    "def detect_drift_all(ref_df, new_df, numerical_cols, categorical_cols, ordinal_cols): \n",
    "    drift_results = {}\n",
    "    \n",
    "    for col in numerical_cols:\n",
    "        drift_results[col] = detect_drift_numerical(ref_df[col], new_df[col])\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        drift_results[col] = detect_drift_categorical(ref_df[col], new_df[col])\n",
    "    \n",
    "    for col in ordinal_cols:\n",
    "        # Treat ordinal as numeric (simple alternative)\n",
    "        drift_results[col] = detect_drift_numerical(ref_df[col], new_df[col])\n",
    "    \n",
    "    return drift_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8884e412-2240-412e-a876-4f5edb6fa481",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333fb202",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
